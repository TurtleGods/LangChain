from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain_core.prompts.chat import(
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate
)
from enum import Enum
from app.config import OPENAI_API_KEY
from langchain.chains import ConversationalRetrievalChain

# ðŸ”¹ ä½¿ç”¨ LangChain ChatOpenAI
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY)
# ðŸ”¹ Enum å®šç¾©
class QueryIntent(str, Enum):
    DETAIL = "detail"
    SIMILARITY = "similarity"
    FILTER = "filter"
    LIST = "list"
    DEFAULT = "default"
    
def get_llm():
    return llm

# ðŸ”¹ Intent åˆ†é¡žå‡½å¼
async def classify_query_intent(question: str) -> QueryIntent:
    """
    ä½¿ç”¨ LLM ä¾†åˆ¤æ–·ä½¿ç”¨è€…çš„æŸ¥è©¢æ„åœ–ï¼Œå›žå‚³ Enum QueryIntent
    """
    # ðŸš€ ä¹¾æ·¨çš„ system prompt
    system_prompt = f"""
        ä½ æ˜¯ä¸€å€‹ Jira æŸ¥è©¢åˆ†é¡žæ¨¡åž‹ã€‚
        å•é¡Œï¼š{question}
        ä½ åªèƒ½å›žè¦†ä»¥ä¸‹å…¶ä¸­ä¸€å€‹å–®å­—ï¼ˆä¸å¾—å¤šå­—ã€ä¸å¾—é™„åŠ èªªæ˜Žï¼‰ï¼š

        {", ".join([e.value for e in QueryIntent if e != QueryIntent.DEFAULT])}, default

        å®šç¾©å¦‚ä¸‹ï¼š
        - detailï¼šä½¿ç”¨è€…æƒ³çŸ¥é“æŸå€‹ Issue çš„ç´°ç¯€ï¼Œä¾‹å¦‚ã€ŒYTHG-830çš„å…§å®¹ã€ã€ã€Œå‘Šè¨´æˆ‘HR-12åšäº†ä»€éº¼ã€
        - similarityï¼šä½¿ç”¨è€…æƒ³æ‰¾ç›¸ä¼¼çš„æ¡ˆä¾‹ï¼Œä¾‹å¦‚ã€Œæœ‰æ²’æœ‰é¡žä¼¼YTHG-830çš„å•é¡Œã€
        - filterï¼šä½¿ç”¨è€…æƒ³æ ¹æ“šæ¢ä»¶æˆ–ç¯©é¸æ‰¾å‡ºç›¸é—œé …ç›®ï¼Œä¾‹å¦‚ã€Œæ‰¾å‡ºè–ªè³‡çµç®—ã€é›¢è·çš„æ¡ˆä¾‹ã€ã€ã€Œcommentæœ‰è§£æ±ºæ–¹æ³•çš„å•é¡Œã€
        - listï¼šä½¿ç”¨è€…æƒ³åˆ—å‡ºå…¨éƒ¨ç›¸é—œé …ç›®ï¼Œä¾‹å¦‚ã€Œåˆ—å‡ºæ‰€æœ‰è¡Œäº‹æ›†ç›¸é—œçš„æ¡ˆä¾‹ã€
        è‹¥ç„¡æ³•æ˜Žç¢ºåˆ†é¡žï¼Œè«‹å›žç­” defaultã€‚
        è«‹åªè¼¸å‡ºä¸Šé¢ Enum çš„å…¶ä¸­ä¸€å€‹å€¼ï¼ˆå°å¯«ï¼‰ã€‚
    """
    prompt = PromptTemplate.from_template(system_prompt)

    llmchain = LLMChain(llm=llm, prompt = prompt)
    # ðŸ”¹ å‘¼å«æ¨¡åž‹
    response = await llmchain.ainvoke({"role": "user", "content": question})
    # ðŸ”¹ å–å‡ºæ–‡å­—å…§å®¹
    content = response['text']

    # ðŸ”¹ é©—è­‰æ˜¯å¦ç‚º Enum æˆå“¡
    return content

async def classify_issue(question:str):
    # ðŸš€ ä¹¾æ·¨çš„ system prompt
    system_prompt = f"""
        ä½ æ˜¯ä¸€å€‹ Jira æŸ¥è©¢åˆ†é¡žæ¨¡åž‹ã€‚
        å•é¡Œï¼š{question}
        è‹¥å•é¡Œä¸­å‡ºç¾ Jira Issue Keyï¼ˆä¾‹å¦‚ YTHG-830ã€HR-12ï¼‰
        è«‹å›žç­”è©²Issue Key
        è‹¥ç„¡æ³•æ˜Žç¢ºåˆ†é¡žï¼Œè«‹å›žç­”None
        """
    prompt = PromptTemplate.from_template(system_prompt)

    llmchain = LLMChain(llm=llm, prompt = prompt)
    # ðŸ”¹ å‘¼å«æ¨¡åž‹
    response = await llmchain.ainvoke({"role": "user", "content": question})
    # ðŸ”¹ å–å‡ºæ–‡å­—å…§å®¹
    content = response['text']
    return content
async def run_chain(Intent:QueryIntent,vectordb):
    qa_system_prompt = get_system_prompt()
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=qa_system_prompt,
    )

    retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 10})

    default_chain = ConversationalRetrievalChain.from_llm(
        llm,
        retriever, 
        combine_docs_chain_kwargs={"prompt": prompt},
        return_source_documents=True)
    
    result = await router_chain(question, default_chain)
    print(result)
    return result["answer"]

def get_system_prompt()-> str:
    prompt = """
        You are a Jira issue assistant. You have access to Jira issues with fields:
        key, summary, description, status.
        response answer in Traditional Chinese.
        Context:
        {context}

        Question:
        {question}
    """
    return prompt