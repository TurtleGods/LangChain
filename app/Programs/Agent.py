from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain_core.prompts.chat import(
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate
)
from enum import Enum
from app.config import OPENAI_API_KEY
# ğŸ”¹ Enum å®šç¾©
class QueryIntent(str, Enum):
    DETAIL = "detail"
    SIMILARITY = "similarity"
    FILTER = "filter"
    LIST = "list"
    DEFAULT = "default"


# ğŸ”¹ Intent åˆ†é¡å‡½å¼
async def classify_query_intent(question: str) -> QueryIntent:
    """
    ä½¿ç”¨ LLM ä¾†åˆ¤æ–·ä½¿ç”¨è€…çš„æŸ¥è©¢æ„åœ–ï¼Œå›å‚³ Enum QueryIntent
    """
    # ğŸš€ ä¹¾æ·¨çš„ system prompt
    system_prompt = f"""
        ä½ æ˜¯ä¸€å€‹ Jira æŸ¥è©¢åˆ†é¡æ¨¡å‹ã€‚
        å•é¡Œï¼š{question}
        ä½ åªèƒ½å›è¦†ä»¥ä¸‹å…¶ä¸­ä¸€å€‹å–®å­—ï¼ˆä¸å¾—å¤šå­—ã€ä¸å¾—é™„åŠ èªªæ˜ï¼‰ï¼š

        {", ".join([e.value for e in QueryIntent if e != QueryIntent.DEFAULT])}, default

        å®šç¾©å¦‚ä¸‹ï¼š
        - detailï¼šä½¿ç”¨è€…æƒ³çŸ¥é“æŸå€‹ Issue çš„ç´°ç¯€ï¼Œä¾‹å¦‚ã€ŒYTHG-830çš„å…§å®¹ã€ã€ã€Œå‘Šè¨´æˆ‘HR-12åšäº†ä»€éº¼ã€
        - similarityï¼šä½¿ç”¨è€…æƒ³æ‰¾ç›¸ä¼¼çš„æ¡ˆä¾‹ï¼Œä¾‹å¦‚ã€Œæœ‰æ²’æœ‰é¡ä¼¼YTHG-830çš„å•é¡Œã€
        - filterï¼šä½¿ç”¨è€…æƒ³æ ¹æ“šæ¢ä»¶æˆ–ç¯©é¸æ‰¾å‡ºç›¸é—œé …ç›®ï¼Œä¾‹å¦‚ã€Œæ‰¾å‡ºè–ªè³‡çµç®—ã€é›¢è·çš„æ¡ˆä¾‹ã€ã€ã€Œcommentæœ‰è§£æ±ºæ–¹æ³•çš„å•é¡Œã€
        - listï¼šä½¿ç”¨è€…æƒ³åˆ—å‡ºå…¨éƒ¨ç›¸é—œé …ç›®ï¼Œä¾‹å¦‚ã€Œåˆ—å‡ºæ‰€æœ‰è¡Œäº‹æ›†ç›¸é—œçš„æ¡ˆä¾‹ã€
        è‹¥ç„¡æ³•æ˜ç¢ºåˆ†é¡ï¼Œè«‹å›ç­” defaultã€‚
        è«‹åªè¼¸å‡ºä¸Šé¢ Enum çš„å…¶ä¸­ä¸€å€‹å€¼ï¼ˆå°å¯«ï¼‰ã€‚
    """
    prompt = PromptTemplate.from_template(system_prompt)
    # ğŸ”¹ ä½¿ç”¨ LangChain ChatOpenAI
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY)
    llmchain = LLMChain(llm=llm, prompt = prompt)
    # ğŸ”¹ å‘¼å«æ¨¡å‹
    response = await llmchain.ainvoke({"role": "user", "content": question})

    print(response)
    # ğŸ”¹ å–å‡ºæ–‡å­—å…§å®¹
    content = response['text']

    # ğŸ”¹ é©—è­‰æ˜¯å¦ç‚º Enum æˆå“¡
    return content
