import os
from app.Programs.router_chain import router_chain
from app.config import OPENAI_API_KEY
from app.services.db_service import get_issue_by_key, load_jira_issues
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import Chroma 
from langchain_core.prompts.chat import(
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate
)
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.schema import HumanMessage, AIMessage
import re
from tqdm import tqdm
from openai import OpenAI

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

async def run_qa(question: str):
    qa_system_prompt = get_system_prompt()
    prompt = PromptTemplate(
        input_variables=["context", "question","issue_key"],
        template=qa_system_prompt,
    )
    # LLM
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY)
    vectordb = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    

    if len(vectordb.get()["ids"]) == 0:
        issues = await load_jira_issues()
        vectordb = build_chroma(issues, embeddings)
    retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 10})

    default_chain = ConversationalRetrievalChain.from_llm(
        llm,
        retriever, 
        combine_docs_chain_kwargs={"prompt": prompt},
        return_source_documents=True)

    issue_key = (
        re.search(r"[A-Z]+-\d+", question, re.IGNORECASE).group(0).upper()
        if re.search(r"[A-Z]+-\d+", question, re.IGNORECASE)
        else None
        )

    print("Start Operating QA Chain...")
    result = await default_chain.ainvoke({"question": question,"issue_key":issue_key ,"chat_history": []})
    
    if 'æ‰¾ä¸åˆ°' in result["answer"]:
        print("âš ï¸ Fallback to RouterChain")
        result = await router_chain(question, default_chain)
    print(result)
    return result["answer"]


def build_chroma(issues, embeddings):
    batch_size=50
    texts = []
    metadatas = []
    for issue in issues:
        text = f"Issue {issue['key']}\nSummary: {issue['summary']}\nDescription: {issue['description']}\nStatus: {issue['status']}\nAssignee: {issue['assignee']}"
        if "comments" in issue:
            for c in issue["comments"]:
                text += f"\nComment by {c['author']} at {c['created']}: {c['body']}"
        texts.append(text)
        metadatas.append({"key": issue["key"]})
    
    vectordb = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    print(f"ğŸ§  Building Chroma in batches of {batch_size}...")
    for i in tqdm(range(0, len(texts), batch_size)):
        batch_texts = texts[i:i + batch_size]
        batch_meta = metadatas[i:i + batch_size]
        vectordb.add_texts(batch_texts, metadatas=batch_meta)
    vectordb.persist()
    print("âœ… Chroma vector DB built and persisted.")
    return vectordb

async def update_chroma(issues):
    if not issues:
        print("ğŸŸ¢ No new issues to update in Chroma.")
        return

    vectordb = Chroma(persist_directory='./chroma_db', embedding_function=embeddings)

    for issue in issues:
        # å°‡æ–‡å­—çµ„æˆä¸€æ®µï¼Œæ–¹ä¾¿ embedding
        text_block = (
            f"Issue {issue['key']}\n"
            f"Summary: {issue['summary']}\n"
            f"Description: {issue['description']}\n"
            f"Status: {issue['status']}\n"
            f"Assignee: {issue['assignee']}"
        )

        if "comments" in issue:
            for c in issue["comments"]:
                text_block += f"\nComment by {c['author']} at {c['created']}: {c['body']}"

        # æ–°å¢æˆ–è¦†è“‹è©² issue çš„å‘é‡è³‡æ–™
        vectordb.add_texts(
            texts=[text_block],
            metadatas=[{"key": issue["key"]}],
            ids=[issue["key"]],
        )

    vectordb.persist()
    print(f"ğŸ’¾ Chroma updated with {len(issues)} new/changed issues.")

# --- å·¥å…·å‡½å¼ ---
def is_exact_issue_query(question: str) -> str | None:
    print("Checking for exact issue query...")
    match = re.findall(r"[A-Z]+-\d+", question)
    if len(match) == 1:
        keywords = ["ç´°ç¯€", "å…§å®¹", "è©³ç´°", "è³‡è¨Š"]
        if any(kw in question for kw in keywords) or question.strip() == match[0]:
            return match[0]
    return None

def extract_issue_key(question: str) -> str | None:
    match = re.findall(r"[A-Z]+-\d+", question)
    return match[0] if match else None


def get_system_prompt()-> str:
    prompt = """
        You are a Jira issue assistant. You have access to Jira issues with fields:
        key, summary, description, status, assignee, created, comments (author, body, created).

        Rules:
        1. If the user asks about a specific issue key (e.g. YTHG-830), return its details (summary, description, comments).
        2. If the user asks for "similar" issues, retrieve semantically related cases using the key's summary/description.
        3. If the user asks about conditions (e.g. salary settlement, resignation, comments with solutions), 
        retrieve relevant issues and filter results accordingly. Summarize if comments contain solutions.
        4. If the user asks to "list all" related issues, enumerate all matching issues with their keys and summaries.
        5. Always keep conversation history in mind for follow-up questions.
        6. Answer in Traditional Chinese.
        7. If nothing relevant is found:
            - If the query contains an issue key like YTHG-830, respond exactly as:
                "æ‰¾ä¸åˆ°è©² {issue_key} çš„ Jira Issue"
            - Otherwise say: "æ‰¾ä¸åˆ°ç›¸é—œçš„ Jira Issue"
        
        Context:
        {context}

        Question:
        {question}
    """
    return prompt
from enum import Enum
class QueryIntent(str, Enum):
    DETAIL = "detail"
    SIMILARITY = "similarity"
    FILTER = "filter"
    LIST = "list"
    DEFAULT = "default"
def classify_query_intent(question: str) -> QueryIntent:
    """
    ä½¿ç”¨ LLM ä¾†åˆ¤æ–·ä½¿ç”¨è€…çš„æŸ¥è©¢æ„åœ–ï¼Œå›å‚³ Enum QueryIntent
    """
    system_prompt = f"""
        ä½ æ˜¯ä¸€å€‹JiraæŸ¥è©¢åˆ†é¡æ¨¡å‹ã€‚
        ä½ åªèƒ½å›è¦†ä»¥ä¸‹å…¶ä¸­ä¸€å€‹å–®å­—ï¼ˆä¸å¾—å¤šå­—ã€ä¸å¾—é™„åŠ èªªæ˜ï¼‰ï¼š

        {", ".join([e.value for e in QueryIntent if e != QueryIntent.DEFAULT])}

        å®šç¾©å¦‚ä¸‹ï¼š
        - detailï¼šä½¿ç”¨è€…æƒ³çŸ¥é“æŸå€‹ Issue çš„ç´°ç¯€ï¼Œä¾‹å¦‚ã€ŒYTHG-830çš„å…§å®¹ã€ã€ã€Œå‘Šè¨´æˆ‘HR-12åšäº†ä»€éº¼ã€
        - similarityï¼šä½¿ç”¨è€…æƒ³æ‰¾ç›¸ä¼¼çš„æ¡ˆä¾‹ï¼Œä¾‹å¦‚ã€Œæœ‰æ²’æœ‰é¡ä¼¼YTHG-830çš„å•é¡Œã€
        - filterï¼šä½¿ç”¨è€…æƒ³æ ¹æ“šæ¢ä»¶æˆ–ç¯©é¸æ‰¾å‡ºç›¸é—œé …ç›®ï¼Œä¾‹å¦‚ã€Œæ‰¾å‡ºè–ªè³‡çµç®—ã€é›¢è·çš„æ¡ˆä¾‹ã€ã€ã€Œcommentæœ‰è§£æ±ºæ–¹æ³•çš„å•é¡Œã€
        - listï¼šä½¿ç”¨è€…æƒ³åˆ—å‡ºå…¨éƒ¨ç›¸é—œé …ç›®ï¼Œä¾‹å¦‚ã€Œåˆ—å‡ºæ‰€æœ‰è¡Œäº‹æ›†ç›¸é—œçš„æ¡ˆä¾‹ã€
        è‹¥ç„¡æ³•æ˜ç¢ºåˆ†é¡ï¼Œè«‹å›ç­” defaultã€‚
        è«‹åªè¼¸å‡ºä¸Šé¢Enumçš„å…¶ä¸­ä¸€å€‹å€¼ï¼ˆå°å¯«ï¼‰ã€‚
    """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY)

    response = llm.ainvoke([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question},
    ])

    content = response

    # é©—è­‰æ˜¯å¦ç‚ºåˆæ³• Enum å€¼
    try:
        return QueryIntent(content)
    except ValueError:
        return QueryIntent.DEFAULT
    
